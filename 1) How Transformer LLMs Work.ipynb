{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Transformer LLMs Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructors: Jay Alammar, Maarten Grootendorst\n",
    "\n",
    "Co-authors of [\"Hands-On Large Language Models\"](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)\n",
    "\n",
    "Official code repo [here](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture was first introduced in the 2017 paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) for machine translation tasks. The idea was like to input an English sentence and have the network output a German sentence. The same architecture tends to be great at inputting a **prompt** and outputting a **response** to that prompt, like a a question and the answer to that question. So it started the rise of **large language models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original transformer architecture consists in two parts an encoder and a decoder. Consider the translation example:\n",
    "\n",
    "- the encoder preprocesses the input text to extract the context needed to perform the translation\n",
    "- the decoder uses that context to generate the translated sentence\n",
    "\n",
    "In language models, the encoder provides rich context-sensitive representations of the input text, and is the basis for the Bert model and most of the embedding models using RAG applications. The decoder performs text generation tasks such as summarizing text, writing code, answering questions and is the basis for most popular LLMs, such as those from OpenAI and Meta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generative model takes in a text prompt and generates a text in response by generating a token at a time. Tokens are words of words' fragments that can be fed into the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the generation process works:\n",
    "\n",
    "- The model starts by mapping each input token into an embedding vector that captures the meaning of that token.\n",
    "\n",
    "- After that, the model parses those token embeddings through a stack of transformer blocks, where each block is a specific NN designed to learn flexibly from the data and scale well on GPUs. \n",
    "    - Each block is made up of an **attention layer** and a feed-forward network.\n",
    "    \n",
    "- The model then uses the output vectors of the transformer blocks and passes them to the language modelling head, which generates the output token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\" width=\"100%\">\n",
    "    <img src=\"images/intro.png\" width=\"400px\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *magic* of LLMs comes not just from the architecture but also from the incredibly rich data that the models learnt from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Language Models: Language as a Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language is a tricky concept for computers since text is unstructured in nature and loses its meaning when represented by zeros and ones or individual characters.\n",
    "\n",
    "Early techniques of language representation were non-transformer, such as **Bag-of-Words** or embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/historic.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words consists in representing text by dividing it in tokens and creating a \"vocabulary\" as a collection of distinct tokens. Then the input text can be represented as a vector with the same length of the vocabulary in which each element represents the frequency of that token in the input.\n",
    "\n",
    "<img src=\"images/bow.png\" width=\"400px\" />\n",
    "\n",
    "Consider that also the tokens that do not appear in the input are represented (as zeros), because a sentence not only gives meaning to the words it contains, but also words it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the B-o-W representation ignores the semantic nature and meaning of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Language Models: (Word) Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Released in 2013, **Word2Vec** was one of the first successful attempts at capturing the meaning of text in embeddings.\n",
    "\n",
    "It learned the semantic representation of words by training on a vast amounts of textual data, like the entire Wikipedia.\n",
    "\n",
    "Word2vec is a framework aimed at learning word embeddings (vector of values) by estimating the likelihood that a given word is surrounded by other words.\n",
    "\n",
    "Starting from a random vector for every token (the embedding), the NN tries to estimate the neighboring tokens and by doing so it \"learns\" the embedding, which captures the meaning of the word. The number of properties or values an embedding has is called the number of dimensions, which can be quite large. In practice, you do not know what the properties exactly represents. However, words with similar meaning share similar embeddings, dimension by dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_embeddings.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models like Word2Vec that convert textual input to embeddings as called \"representation models\" as they attempt to represent text as values.\n",
    "\n",
    "Similar techniques can be used for entire sentences, to create sentence embeddings, and the same for longer texts such as documents to create document embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/types_of_embeddings.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above the word \"vocalization\" is split into two tokens, then, after passing through the representation model one can average the two embeddings  to get the meaning of the original word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Language Models: Encoding and Decoding Context with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flaw of Word2Vec is that it creates static embeddings: the same embedding is generated for the same word regardless of the context.\n",
    "\n",
    "An improvement was to use **Recurrent Neural Networks (RNNs)** to model entire sequence of words. They are used for **encoding**, that is representing an input sentence in one embedding, and **decoding**, that is to generate the output sentence.\n",
    "\n",
    "Each word of the output is generated in an autoregressive way since it uses all input and the previously generated output words.\n",
    "\n",
    "The input words are first tokenized using for example the Word2Vec representation, they are fed simultaneously into the encoder to generate the context in the form of an embedding. The decoder uses this context embedding to generate the outputs (one at a time using the previously generated words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a single context embedding was problematic to capture the entire context of a long sentence. In 2014 a solution called **\"attention\"** was introduced. It allows a model to focus on parts of the input that are relevant to one another. It selects which words are important in a given sentence to generate an output.\n",
    "\n",
    "Instead of passing only a context embedding to the decoder we pass the hidden states of all input words. The decoder then uses the attention mechanism to look at the entire sequence but highlighting words that are more relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/attention_model.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the sequential nature of this architecture is that it precludes parallelization during the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Language Models: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true power of attention was first explored in the [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) 2017 paper. This paper introduced the **transformers** architecture which is based solely on attention without the RNN. This architecture allows the model to be trained in parallel which speeds up calculation significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer consists in stacked encoder and decoders blocks.\n",
    "\n",
    "In the encoder the input is converted to embeddings (starting with random values). Then **self-attention**, which is attention focused only on the input, processes these embeddings and updates them. The updated embeddings contain more contextualized information as a result of the attention mechanism. They are then passed to a feed-forward NN to finally create contextualized token word embeddings.\n",
    "\n",
    "<img src=\"images/encoder.png\" width=\"500px\" />\n",
    "\n",
    "Notice that the encoder is meant for representing text and does a good job in generating embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the encoder has processed the information the decoder can now take the previously generated words and pass them to the **masked self-attention** (similar to the encoding part). These intermediate embedding are now generated and passed to another attention network together with the embeddings of the encoder, thus processing both what has been generated and what you already have. This output is again passed to a feed-forward NN and finally generates the next word in the sequence.\n",
    "\n",
    "<img src=\"images/decoder.png\" width=\"500px\" />\n",
    "\n",
    "Mask self-attention masks future positions so that any given token can only attend to tokens that came before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, a new architecture called **Bidirectional Encoder Representations from Transformers (BERT)** was introduced. It has more application beside translation than the first transformer.\n",
    "\n",
    "Bert is an encoder only architecture that focuses on representing language and generating contextualized word embeddings.\n",
    "\n",
    "The encoder blocks are the same as before (self-attention + NN) but the input contains an additional \"classification\" (CLS) token which is used as a representation for the entire input. This CLS token is often used for fine-tuning the model on specific tasks like classification.\n",
    "\n",
    "<img src=\"images/bert.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model like BERT you use a technique called masked language modelling in which you first randomly mask a number of words from the input sequence and have the model predict these masked words. By doing so the model learns to represent language as it attempts to deconstruct these masked words.\n",
    "\n",
    "<img src=\"images/bert_train.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is typically a two-step approach:\n",
    "\n",
    "- First you apply masked  language modelling on a very large amount of data, and this is called \"pre-training\".\n",
    "\n",
    "- After which you can fine-tune the pre-trained model on different downstream tasks such as classification, name entity recognition, paraphrase identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the other side, **generative models** such as **Generative Pre-Trained transformer (GPT-1)** only use a decoder with masked self-attention and a NN to generate the next word.\n",
    "\n",
    "<img src=\"images/GPT1.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can have a maximum \"context length\", which is the maximum number of tokens they can process at a given time. It is made by both the input tokens and the previously generated tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-1 had 117 million parameters, GPT-3 already reached 175 billions parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The year 2024 has been the year of generative AI (so far), with many proprietary and open-source models:\n",
    "\n",
    "<img src=\"images/2024.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important intuition to understand how transformers work is to understand that it generates tokens one by one. Moreover, every generated token depends also on the previously generated ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer is made of three major components:\n",
    "\n",
    "- Tokenizer: the component that breaks down the text into multiple chunks.\n",
    "- Stack of Transformer Blocks: the NN where the computing is.\n",
    "    - The model has an embedding associated to every token which substitute the token in the beginning once the model is processing its inputs\n",
    "- Language Modelling Head: another NN that gives a probability distribution on all the tokens. From this token probability there are many *decoding strategies*:\n",
    "    - One can then choose the token with the highest probability (greedy decoding, `temperature = 0`)\n",
    "    - Alternatively one can add randomness among the tokens with the highest probability (`temperature > 0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another intuition that makes transformers better than RNN is that they process all of their input tokens in parallel, and that parallelization is time efficient.\n",
    "\n",
    "The generated token in decoder LLM transformers is the output of the final token in the model. Once the first token is generated we feed the entire prompt with the generated token in the transformer again. However, this time the calculation is cached to speed up the computation (KV caching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/archi.png\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each transformer block is made of two major components:\n",
    "\n",
    "- Feed-Forward Neural Network (latter)\n",
    "    - Intuitively it is a storage on information and statistics and it is able to estimate the most probable token after the ones already generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_block.jpg\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self-Attention Layer (former)\n",
    "    - Allows the model to attend to previous tokens and to incorporate the context in its understanding of the token it is currently looking at.\n",
    "        - It first assigns a score to how relevant each of the input tokens are to the token currently processed (\"**relevance scoring**\").\n",
    "        - It then combines the relevant information into the representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_block_2.jpg\" width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention happens in what it is called an *attention head*, and it is conducted using three matrices called Query, Key and Value Projection matrices, made by parameters to be estimated and the tokens (both previous and current). Intuitively each rows of these matrices is a vector representing a token of the series.\n",
    "\n",
    "The end goal of **relevance scoring**  is to assign a score to every previous token to determine which are the most relevant for the token currently being processed. This is done combining the row of the current token in the Query matrix and the whole Key matrix. The scoring is then combined with the Value Matrix to get weighted values for each of the previous tokens. We then sum the weighted values and get the output of the *attention head*.\n",
    "\n",
    "<img src=\"images/self_attention.png\" width=\"600px\" />\n",
    "\n",
    "For details on the calculation see the course [Attention in Transformers: Concepts and Code in PyTorch](https://learn.deeplearning.ai/courses/attention-in-transformers-concepts-and-code-in-pytorch/lesson/han2t/introduction?courseName=attention-in-transformers-concepts-and-code-in-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In self-attention the calculation happens in parallel in multiple attention heads, where each attention head has its own set of keys, queries and values weight matrices. Then the information is combined across all heads.\n",
    "\n",
    "<img src=\"images/self_attention_base.jpg\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More efficient strategies are to use the same keys and values so that only the queries are specific for every head, with a lower number of parameters to be estimated. This is approach is called *Multi-Query attention*.\n",
    "\n",
    "More recently, *grouped query attention* is an efficient attention mechanism that allows to use multiple keys and values for different groups of attention heads, leading to better results than just sharing the same matrices of keys and values across all heads.\n",
    "\n",
    "<img src=\"images/grouped_attention.jpg\" width=\"500px\" />\n",
    "\n",
    "\n",
    "So that if a model uses *Multi-Query attention* it will state how many heads and how many groups it is using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important recent idea for improving the efficiency of attention is the idea of *sparse attention*, so that a token only looks back at a subsample of the previous ones. Local attention boosts performance by only paying attention to a small number of previous positions. See the paper [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More recently, and to allow these models to go through 100,000 or 1 million token context sizes are ideas like [*ring attention*](https://coconut-mode.com/posts/ring-attention/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recent Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper from 2017 author used **positional encoding** which is a technique used in transformers to apply positional information since tokens are processed in parallel but the order of the words in the sequence matters. It is applied right before entering the transformer block.\n",
    "\n",
    "In a more modern version (2024) positional encoding is no longer used. The idea has been substituted by **rotary embeddings** and positional information is now added at the self-attention level. Moreover, layer normalization has been moved before the self-attention and feed-forward NN because some experimental results showed that the models do better with this setup.\n",
    "\n",
    "Both versions use **residual connections** that go around the layers to repack the information from the beginning of the layer.\n",
    "\n",
    "<img src=\"images/modern_transformer.jpg\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more recent development in LLMs is the idea of Mixture of Experts (MoE). The idea is that instead of the single feed-forward NN, at each layer you have multiple sub-neural networks, each one called an *expert*. And then a *router* in each layer which decides which expert should process the token. The *router* works as a classification score and chooses the best *expert* in each layer. It is a small feed-forward NN itself, with the goal to create a probability score to indicate how likely is that the expert is suited for that particular input. It can choose the best expert or mixing many experts and then aggregate their output using a weighted mean\n",
    "\n",
    "Experts might tend to focus on specific kind of tokens such as punctuation (, . : & ?, etc...), verbs, conjunctions (the, and, if, not, etc...), visual descriptions (dark, outer, yellow, etc...) and focus on how to process them.\n",
    "\n",
    "<img src=\"images/moe.jpg\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional feed-forward NN is called a *dense network*, being the largest component of the LLM, since all parameters are activated and used (at least to some degree) to find complex relationships in the information processed by the attention mechanism.\n",
    "\n",
    "On the contrary, with MoE there are many networks. When the input flows through the expert layer, one or more experts is selected that will process the inputs, and the rest of the experts will remain inactivated. This is called a *sparse model* since only a subset of experts is activated at a given time.\n",
    "\n",
    "<img src=\"images/moe_detail.jpg\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major benefit of MoE is its computational requirement. From one side you have more parameters to load (one set of parameter for each expert), but only the parameters of the active experts are used in inference. For example, in the model *Mixtral 8x7B* there are 8 experts with 5,6B parameters each, for a total of 45B parameters among the experts. However, when the model runs only 2 experts are used together, resulting in using \"only\" 11B parameters.\n",
    "\n",
    "<img src=\"images/mixtral.jpg\" width=\"700px\" />\n",
    "\n",
    "Although the sparse parameters might seem large, during inference the model actually uses much fewer resources. This makes MoE models excellent when you run them in production. Moreover, performance tends to be higher than traditional models, however there is the risk of overfitting on a single expert and should be trained carefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv37tf",
   "language": "python",
   "name": "myenv37tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
